**Research Strategy**

**Significance**

Successful human locomotion relies on robust communication between the
visual and motor systems: the visual system finds and transfers relevant
information about the world to the motor system, resulting in the finely
tuned control of foot placement. An outstanding complexity of the
*visuo-locomotor* system is understanding precisely how upcoming
footholds are found during visual search. Independently, the study of
visual search and the biomechanics of locomotion have both received a
great deal of attention^1,2^, but there is little research addressing
their critical overlap. The lack of interdisciplinary research not only
hinders progress in the independent fields of vision science and
biomechanics, but additionally restricts our understanding of how the
visuo-locomotor system might be affected by retinal^3^ and motor
diseases.

![](media/image1.jpeg){width="7.551388888888889in"
height="3.327777777777778in"}Recently, the sponsor of this proposal (Dr.
Matthis) and his colleagues conducted the first investigation into eye
movements made while traversing natural outdoor terrains of varying
difficulty^4,5^ (*Fig. 1*). Examination of this data demonstrates that
eye movements made during visual search might be influenced by the
biomechanical constraints of the locomotor system, such that
participants fixate where they *want to step* as opposed to visually
salient terrain. Critically, this observation -- that the proprioceptive
information of biomechanics might influence visual search -- is
substantiated by the current state of the visual search and
biomechanical literatures.

There is strong evidence from investigations into visual search for
targets within natural scene statistics^6^ that search patterns are
highly fixation-efficient -- as modeled by an *ideal observer* -- where
subsequent eye movements are made to maximize the probability of finding
the target. Additionally, there is a body of research which demonstrates
that visual search is highly task-dependent^7--12^. This research
demonstrates that the visual system works to provide information
relevant to the goals of the perceiver, such that eye movements are
efficiently made to serve the current task demands and reduce
uncertainty^4^. What information is most relevant to the locomotor
system?

Examinations of bipedal biomechanics consistently demonstrate that human
locomotion is energetically efficient^1,13^. Recently, researchers have
shown that the humans are constantly adapting their gait to maintain
energetically optimal movements even seconds after perturbation^14^.
This, combined with evidence that humans can readily perceive
energetically efficient^15^ or optimal^16^ body movements, suggests that
the visual system must be seeking information which allows for
energetically efficient foot placements.

**[In this proposal we will determine the influence of biomechanical
constraints in visual search strategies while walking across complex
terrain]{.underline}**. To achieve this, we must investigate the
visuo-locomotor system using a new paradigm designed to understand
visual search in the unique context of locomotion, by manipulating
explicit task-constraints (**Aim 1**) and visual search difficulty
(**Aim 2**). Additionally, we will go outdoors in **Aim 3** to
specifically investigate how visual search strategies might incorporate
biomechanical information in natural environments.

**[Innovation:]{.underline}** Previous laboratory investigations into
the visual control of foot placement have been greatly restricted by the
length of the walkable space^17--19^, where only a few steps are
recordable in a given trial and (in some cases) no feedback is provided
to the participant to tell them if their foot placement was successful.
Additionally, some of the best work investigating the visual control of
foot placement took place at a time where there was a technological
inability to measure precise eye movements in relation to the gait cycle
in real time. Here, we develop our new experimental paradigm using an
indoor, 14m long (3m wide) projector-based Augmented Reality (AR)
groundplane (*Fig. 2*, *Fig. 3*), which participants will actively walk
across, enabling us to measure \~25 footsteps made in a given trial. We
will measure participant eye movements with a mobile, binocular eye
tracker (Pupil Labs), as well as their full body kinematics and position
tracking using maker based motion capture (Qualisys). By combining new
technologies with a much larger laboratory space (as well as an improved
capacity to measure visuo-locomotor in natural, outdoor terrains), we
provide the first experimental designs which will be sufficient to
understand the influence of biomechanical constraint in visual search.

![](media/image2.png){width="4.631944444444445in"
height="5.055555555555555in"}**[Objectives:]{.underline}**

In **Aim 1**, we target the *task-dependency* of visual search by
manipulating the instructions participants receive while engaging with
the same, 14m long, projected groundplane stimulus. In [experiment
1]{.underline}, participants will receive different instructions in
three different experimental blocks: 1. Standing still while watching
the groundplane flow toward them, 2. Walking across the groundplane,
searching for target circles, or 3. Walking across the groundplane while
only stepping on circles and avoiding Landolt C's (*Fig.* 1). By
measuring how visual search patterns change across these three
task-dependent conditions, we will be able to parse influence that might
be driven by self-motion, biomechanical constraint, and the fixation
efficiency of investigating visually salient terrain features.

In **Aim 2**, we investigate the hypothesis that the difficulty of
visual search (acuity of distractors) will force participants to use an
alternative foothold planning strategy. Investigation of eye movements
made while traversing outdoor terrain^4,5^ demonstrates that as terrain
difficulty increases, the more precise information about the second and
third upcoming foothold is required for appropriate step planning. It is
unclear, however, if this is driven by the difficulty of the terrain, or
alternatively the difficulty of the visual search task itself. As visual
search difficulty increases, and more certainty is required out of each
fixation, are more careful visual search patterns adapted, like what has
been observed in rough terrains^4,5^? We address this question in
[experiment 2]{.underline}: Participants will walk across our AR
projected terrain, where they will be tasked with only stepping on
circles and avoiding C's, and from trial to trial we will manipulate the
acuity of the distractor "Landolt C", thus increasing the difficulty of
identifying circle footholds^20^.

In **Aim 3** we observe the visuo-locomotor system in natural outdoor
terrains in unprecedented detail, combining binocular eye tracking,
IMU-based motion capture, and 3-D terrain Photogrammetry mapping (*Fig.
4*). By combining these three different methodologies, we will be able
to measure visual search patterns made during locomotion of complex, 3-D
terrains, mapping where precise eye movements fall onto complex 3-D
objects (e.g., rocks). This exploratory aim explores the
generalizability of the previous two aims, attempting to demonstrate
that biomechanical information influences visual search in natural
environments as well as laboratory settings.

**Approach**

***Aim 1. Manipulate task-constraints to investigate visual search
strategies in pure-visual vs. locomotor-constrained visual search tasks,
with and without self-motion.***

**Experiment 1.** **How do explicit task demands elicit change in visual
search patterns?**

*[Rationale]{.underline}*: Previous studies examining patterns of visual
search have shown that an ideal observer model is capable of replicating
search patterns while searching for Gabor patches in 1/*f* noise^6^.
This study demonstrates that individual eye movements seek to maximize
the information gained with each fixation. Additionally, there is a
wealth of research from Hayhoe and colleagues which further demonstrates
that visual search is highly information-efficient^7--11^ (i.e.,
uncertainty reducing) and task dependent; eye movements made in natural
environments seek the most pertinent information for the task at hand.
Given that the locomotor system optimizes for energetic efficiency, it
is plausible that during locomotion that the visual system uses relevant
biomechanical information to aide the visual search process. Under what
circumstances does visual search incorporate influence from the
biomechanical constraints? Is it the task of looking for
target-footholds which will impose biomechanical influence, or is it the
act of self-motion itself?

Self-motion by alone might be sufficient to recruit neural mechanisms
that influence visual search patterns: there is strong evidence that
many motor systems (and perhaps the oculomotor system) are *softly
assembled*^21^, meaning that the components of the system are not
"hard-coded", but rather are flexible, recruiting different muscles or
neural systems based on task demands^22^. In experiment 1, we
investigate layers of task demands in three different within-subjects
conditions: Condition 1 -- visual search while standing still; Condition
2 -- visual search while walking straight forward; Condition 3 -- visual
search for footholds.

We hypothesize that visual system is optimizing for the locomotor task
(condition 3) -- where targets that could serve as footholds will
attract fixations, and other potential targets will be ignored. When the
same targets are merely objectives for visual search (conditions 1 and
2), independent of biomechanical constraint, participant visual search
patterns will stretch beyond targets that they might step on. While it
is possible that self-motion (condition 2) could impose biomechanical
influence on the search process, we anticipate that because the targets
are independent from where the participant is stepping, that
biomechanical information will not influence visual search in this
condition.

*[Methods and Stimuli]{.underline}*: We will use pilot data to conduct a
power analysis to assess the number of subjects required for this study
-- anticipating that 6 subjects will be sufficient^4^. During the
experiment, we will record each participant's binocular eye movements
(Pupil Labs) and full body kinematics (Qualisys) over approximately one
hour and thirty minutes of data collection. Prior to the start of the
experiment, 10 free walking trials will be recorded to serve as a
baseline measure for each participant's preferred gait cycle and
biomechanics -- where participants are asked to simply walk at their
preferred walking speed across the 14m tracking space ten times. Each
subject will participate in three 30 minute condition blocks, presented
in random order. The groundplane stimulus will be made up of target
circles and distractor Landolt C's (*Fig. 2*) (diameter and line width
for both circles and C's: 20cm diameter, 5cm line width). The precise
ratio of target-Circles to distractor-Landolt C's per square meter will
be determined in a pilot study such that the visual search task is
challenging, but also so that the participants can successfully walk
across the 3x14m groundplane without stopping or making more than 2
missteps. The stimuli will be the repeated in the three different
blocks, but the task-instructions given to our participants will change.
There will be 60 trials in each condition, for a total of 180 trials per
subject.

*Condition 1*: Participants will be told to stand at the beginning of
the AR projector terrain, and that their only task is to look for
circles among distractor C's (Landolt C's), and to click a button on a
remote clicker indicating that they are fixating on a circle (target).
In this condition (1), the participants will stand still at the
beginning of the ground plane display as the stimulus flows towards them
at the speed which the terrain would move if they were walking
comfortably (measured during the 10 free-walking trials for each subject
before the experiment begins).

*Condition 2*: Participants will be given the same stimuli and search
instructions as in Condition 1, but now are told that they must walk at
a comfortable walking speed across the stimulus to the other side of the
room, indicating when they find a circle with their remote clicker.

*Condition 3*: Participants will be given the same stimuli as in
Condition 1, but now are told that they are required to cross the 14m
walkway by only stepping on circles. Participants will receive auditory
feedback for each step (happy beeps for successful foot placement,
buzzer sound for a miss) and will have to repeat the trial if they have
three or more missteps. Participants will not use the clicker in this
condition.

While the clicker data in condition 1 and 2 will be collected and
observed, it serves primarily to engage our subjects with the visual
search task; no specific hypotheses have been formed around differences
between clicker responses in the two conditions.

*[Data Analysis]{.underline}*: For our initial analysis, we will first
time match the eye movement and kinematic data. We will perform an
analysis on the motion capture data to identify the current phase of
gait in time (double support, early swing, or late swing; in Condition
1, the gait will be identified simply as "standing"), so that we can
categorize fixations relative to these three phases of the gait cycle.
Additionally, by combining the eye movement and motion capture datasets,
we will be able to calculate where the participant was in the room, and
where their 3-D gaze vector intersects with the groundplane (a la *Fig.
1*). Fixations will be defined as moments the world-relative gaze
position is maintained for greater than 100ms (fixation length
determined by examination of eye movements made in natural
terrains^4,5^). We will treat every fixation as a sample marking the
search time (time since a previous target fixation), phase of gait, and
the task condition in which the fixation occurred. We will map the
fixation data in two different reference frames: world-centered and
retinotopic (polar coordinates: amplitude, direction) space. We will
conduct two analyses to assess the potential influence of biomechanical
information on patterns of visual search.

Our first analysis will focus on fixations made in world-centered
coordinates where we calculate the distance between each fixation and
the nearest biomechanically preferred foothold (in Condition 1, this
will be the location of the subjects first step). Three biomechanically
preferred footholds will be estimated using the previous two footholds
and predictions based on a powered walking model^1^. Fixation distance
to preferred foothold (DtPF) will serve as our dependent variable. We
will then analyze DtPF using a linear mixed effects regression (LMER)
model, regressing the task condition, time from last target fixation,
and the phase of the gait cycle onto DtPF -- where individual trial and
subject variance is accounted for in the random effects structure. If
our hypothesis is correct that biomechanical information will be
utilized in condition 3 but not in conditions 1 and 2, then we should
see a significant decrease in DtPF relative to conditions 1 and 2.
Additionally, if biomechanically preferred footholds are used at the
onset of the visual search process, we should see in condition 3 that,
relative to search time, that the DtPF increases as time from previous
target fixation increases. Finally, if there is an effect of self-motion
on the utilization of biomechanical information, we should see a
significant difference between DtPF in condition 2 relative to condition
1.

![](media/image3.jpeg){width="3.5in" height="3.3645833333333335in"}Our
second analysis will assess the general shape of visual search
strategies in retinotopic coordinates, where each saccades amplitude and
direction will be recorded along with the corresponding task-condition
and the phase of the gait cycle during which the saccade was made.
Saccade amplitudes and directions will be analyzed using similar LMER
structure in the previous analysis. We hypothesize that the overall
shape of visual search strategies, i.e. the statistics of the amplitude
and direction, will be strongly reflected by the task demands: In
conditions 1 and 2, where subjects are free of biomechanical constraint,
we expect larger amplitudes and variable saccade directions compared to
condition 3, where the visual search strategy will be constrained by
serving locomotor demands. Additionally, in condition 3 but not
conditions 1 and 2, we anticipate that saccade direction will have a
meaningful relationship to the current phase of the gait cycle. Saccade
directions in condition 2 should be independent of the gait cycle
because visual information is independent from biomechanical constraint
-- but if pure self-motion is sufficient to activate biomechanical
influence, we should see a meaningful relationship between saccade
direction and the gait cycle in this condition as well.

*[Potential Problems & Solutions]{.underline}*: Properly segmenting gaze
relative to the gait cycle, measuring time relative to target fixations,
as well as calibrating independent pieces of equipment is
methodologically nontrivial. Given the combined technical expertise of
Dr. Matthis and myself, where Dr. Matthis has extensive experience
calibrating multiple measurement systems and the two of us have a
combined 18 years of experience with Augmented/Virtual Reality
experiments, we are confident in our ability to conduct the research as
described.

***Aim 2.** **Parametrize the relationship between eye movements and
locomotor constraints by manipulating the difficulty of the visual
search. ***

**Experiment 2. Measure the relationship between visual search
difficulty and biomechanically informed route planning strategies.**

*[Rationale]{.underline}*: Previous laboratory studies have demonstrated
that humans tend to look around two steps ahead while walking, and that
this look-ahead window provides the visuo-locomotor system with the
information it needs to actively adjust the biomechanics of locomotion
efficiently^17--19,23,24^. More recent work examining visuo-locomotor
behavior in natural terrains^4,5^ shows how the foothold that
participants tend to fixate on changes relative to the difficulty of the
terrains. Matthis et al.^4^ show pertinent differences between medium
and rough terrain conditions: participants are most likely to fixate
around the second upcoming foothold in medium terrains, but gaze
probability is evenly shared between the second and third upcoming
foothold for the rough terrains. This finding suggests a greater need
for path planning in difficult terrain, with fewer viable footholds. It
is unclear if a different gaze-allocation strategy -- i.e. visual search
pattern -- is adopted because of the difficulty of the terrain itself,
or the difficulty of the visual search task.

Here, we seek to understand the influence of visual search difficulty on
foothold searching strategies by having participants walk across an AR
projected groundplane, like condition 3 of experiment 1, where they can
only step on circles. To manipulate the difficulty of visual search, we
decide to manipulate the acuity of the Landolt C distractor by changing
the size of the "C" gap, thus manipulating the difficulty of
discriminating between targets and distractors.

*[Methods and Stimuli]{.underline}*: The number of participants
recruited for this study will be based on a power analysis from a pilot
study. During the experiment, we will record each participant's
binocular eye movements (Pupil Labs) and full body kinematics (Qualisys)
over approximately one hour of data collection. Prior to the start of
the experiment, 10 free walking trials will be recorded to serve as a
baseline measure for each participant's preferred gait cycle and
biomechanics. In the pilot study, we will identify the upper boundaries
of the acuity of the "C" gap which a participant can perceive while
walking (without stopping) across our groundplane stimulus, using that
as the ceiling for our experiment and having a half-circle "C" as the
easiest acuity condition. We will manipulate the acuity in experiment 2
by sampling 10 values in mm that are evenly distributed from the most
difficult acuity condition to the easiest, so that we can sample from a
psychometric function. We will then repeat each of the 10 acuity
conditions 12 times, providing 120 recorded trials. Participants will be
shown 130 trials total, including the free walking trials. The 120
experimental trials will be pre-generated based on the "C" gap (mm) for
that trial.

At the beginning of every trial, participants will be told to cross the
AR terrain while only stepping on Circles, avoiding Landolt C's, and
that the trial will end when they have successfully navigated to the end
of the AR-walkway. They will also be told that if they misstep (defined
as placing their foot on anything but a target circle) three times, they
must restart the trial. Participants will receive real-time auditory
feedback, positive for foothold successes (happy beeps) and negative for
missteps (buzzer sound). Foothold configurations will be vetted prior to
the experiment to be sure that none of them are prone to excessive
failures. Once the participant has reached the other side of the room,
the trial ends and the next configuration of footholds will appear.

*[Data Analysis]{.underline}*: The primary analysis of this dataset will
consider fixations made in a world-centered frame of reference, so that
we can precisely identify the relationships between fixations and
upcoming footholds (identified in post-processing) in time and space,
assessing how this relationship might change based on the difficulty of
the visual search task. To test this, we will calculate the distance
between each fixation and its nearest upcoming foothold, seeing how gaze
distribution changes relative to the acuity of the distractor "C". We
predict that as acuity increases, participants will spend a greater
amount of time looking at footholds that are further ahead (3+) when
compared to when the discrimination task to find target foothold circles
is easy, where participants will be most likely to look at the second
upcoming foothold. Gaze distributions will be compared using Bayes
T-tests.

We will also fit a psychometric function of participant walking speed
and visual acuity: as visual search difficulty increases, and the
participant needs longer fixations to increase certainty of each
foothold, we should be able to find a critical value of acuity such that
movement speed decreases dramatically. This represents the visual system
informing the body that it needs more time to be certain --
demonstrating another aspect of the role of the body in visual search.

*[Potential Problems & Solutions]{.underline}*: Identifying the best
manipulation to change the visual search difficulty is difficult,
especially due to the novelty of this paradigm. Landolt C's were chosen
because of how easy they are to parameterize, but other stimuli have
been considered and could be used instead (e.g., Gabor patches hidden in
1/f noise^6^).

![](media/image5.png){width="4.180555555555555in"
height="2.2305555555555556in"}***Aim 3. Observe the visuo-locomotor
system in natural, unconstrained environments***

**Experiment 3: Do locomotor constraints influence visual search in
natural environments?**

*[Rationale]{.underline}*: A critical test of how well our experimental
insights generalize to locomotion across complex terrains in natural
environments is to attempt to show that the biomechanical constraints
influence visual search in the real world.

When measuring walking in outdoor terrains, can we use a dynamic walking
model to predict participants biomechanically preferred foothold, and
does this point in retinotopic space drive visual search behavior? How
might the natural constraints of terrain difficulty interact with where
participants would prefer to step?

Furthermore, experiment 3 will provide us with opportunities to assess
where our previous experiments fail to equip us with the understanding
needed to understand the full scope of visuo-locomotor behavior, likely
due to complexity of 3-D terrain which has highly variable, uncontrolled
visual information. From this experiment, we will be able to generate
hypotheses to bring back into the laboratory, testing new visual
components of the visuo-locomotor system that previously went
unexplored.

*[Methods & Terrain]{.underline}*: We will employ methods similar to
previous research conducted by the sponsor^4^, collecting data from 6
subjects exploring various terrain in publicly available (Blue Hills
Reservation, MA; Beavertail State Park, RI) outdoor environments outdoor
environments: flat, medium, and rough (rough example, *Fig. 1*). Using
IMU-based motion capture (Motion Shadow) and mobile binocular eye
tracking (Pupil Labs), we will combine body and eye information with a
reconstructed 3-D terrain map (post-processed) generated by a combined
images from a variety of go-pro cameras which will be set up around the
outdoor tracking spaces. In each terrain condition, participants will
walk for approximately 400m in a roughly straight trajectory; to obtain
this outdoors, participants might have to walk over the same area
multiple times. We will ensure that there is a comparable amount of data
in all three terrain conditions.

*[Data Analysis]{.underline}*: Our primary analysis of this dataset will
be similar to our analysis of the distance to preferred foothold (DtPF)
in experiment 1. By using 3-D terrain reconstruction, we will be able to
map fixations in 3-D space, and calculate the distance between each
fixation and the preferred upcoming foothold(s). As terrain becomes more
complicated, that and the momentum of the walker varies more greatly
Medial-Laterally, that the preferred next foothold will change from what
one might expect; rather than being straight ahead, based on the past
couple steps, it might be biomechanically preferable to step to the
side. We predict then that biomechanically preferred foothold locations
will change greatly with the difficulty of the terrain, but eye
movements will still be driven by the preferred biomechanics as long as
there is some level of uncertainty involved with the task of finding
viable footholds (e.g., in medium and rough conditions, but not in
easy). DtPF will be analyzed using an LMER analysis, regressing the
terrain difficulty and the phase of the gait cycle (double support,
early swing, or late swing), onto DtPF.

*[Potential Problems & Solutions]{.underline}*: Accurately predicting
the biomechanically preferred foothold in rough terrain could be
difficult, and is a substantial challenge within this aim. If
calculating the body's precise preferred foot placement proves too
complex, we can alternatively use the body's momentum instead, assessing
how that might drive saccade directions. Additionally, conducting high
quality research in natural environments is difficult -- but Dr. Matthis
has experience doing precisely this. 3-D reconstruction of the
environment should also prove challenging, but Dr. Matthis's lab (the
Human Movement Neuroscience lab) employs Northeastern Co-Op students in
computer graphics and image processing, students who will provide
critical expertise in the reconstruction of 3-D environments.
