
NOT CHANGED YET

\textbf{\underline{Rationale:}}  The main goal of this aim is to is to jointly describe and model visual perception, visual selection (gaze), and biomechanics (gait) in the context of the natural behavior of walking in real-world environments.  Ultimately, this work will provide an integrated account of the sensorimotor processing underlying visually-guided walking.

We will measure and model the specific fixation strategies used by walkers during foothold finding.  We will evaluate how these fixation strategies are influenced by the biomechanical constraints of walking. What visual and biomechanical information is most predictive of the location of the next footfall and/or the next gaze location?  How is that information extracted from the retinocentric reference frame of the walker?

Through the proposed divided attention conditions, we seek to understand the tradeoffs that walkers make to maintain stability and find footholds while engaging in other tasks.  How do they modify their fixation patterns as they split their attention between tasks?  Are there stereotyped ways in which all walkers split their fixation, suggesting that certain fixation sequences are chunked?

\noindent\underline{\textbf{Technical Deliverables:}} A major contribution of this grant will be to lower the barrier to the collection and analysis of integrated visuomotor datasets collected in real-world environments.  The is will involve the production of open-hardware, open source data processing pipelines, and a well-documented visuomotor dataset for visually-guided walking:

[Hardware / Data Collection Documentation] We will provide detailed open source documentation for the hardware integration of a motion capture suit (Rokoko), eye tracker (Pupil Labs), and data collection device (tablet/computer). We will also create detailed documentation and video tutorials for how to collect these data.

[Data Processing Pipelines] There are several steps required to convert the raw data from these devices into a calibrated and integrated visuomotor dataset. In our previous work together, we created an initial version of these pipelines. However they are not easily re-usable by other groups or updated for new technologies. Since the creation of those initial pipelines, Matthis (NU)
has had significant training in open source software development. Using our internal pipelines as a starting point we will develop a suite of packages that: 1. Perform photogrammetry on the world video content, relying on Meshroom (https://alicevision.org/). 2. Spatially calibrate the gaze data, body motion and environment into the same reference frame (see methods CITE). 3. Generate visualizations of walkers moving through the terrain (e.g., youtube link). All three packages will be written in
python and shared as open-source packages on Github.  These will become a vital resource for the study of sensorimotor processing.  

[Data] The integrated visuomotor dataset (n=75) collected in Aim 1 will be made available as a data repository on Zenodo. This dataset will include the original raw data as well as the fully processed data that includes the photogrammetry reconstruction and spatially calibrated data.  We will work internally and with collaborators (e.g., Johannes Burge, Eero Simoncelli, ... ?? we could get letters of support) to build an API that not only allows individuals to download the full dataset, but also provides methods to query the dataset for combinations of image, video, biomechanics, and label data well-suited to advanced modelling/ML that is beyond the scope of the current proposal.

\noindent\underline{\textbf{Research Design:}} The resulting data will provide the opportunity to answer a variety questions about visual perception, gaze, gait, the environment, and the coordination of these during walking.

i. \emph{How do walkers allocate gaze (spatially and temporally) during foothold finding? What is the planning horizon?}

Previous work has established that people allocate their gaze to locations 2-5 footholds ahead during foothold finding [CITE], shown also in Figure XXx. These findings did not have the precision afforded by the photogrammetry pipeline described above. Preliminary analyses confirm previous conclusions that walkers are allocating gaze to upcoming footholds, as the increased precision leads to the emergence of ``hills of gaze'',
Figure XXx\ldots{} For each individual we will quantify the allocation of gaze to upcoming footholds, establishing the variability in gaze allocation across a larger population of individuals. The increased precision also allows the classification of gaze locations by their
foothold, or as a non-foothold search location, further distinguishing between non-foothold search locations near the path and those that are part of search sequences before turns (i.e., paths not taken; see Figure XXx [sequence of gaze locations, classified]). For each individual we will quantify the \% time they spend on search locations that become
footholds, the \% time they spend on search locations that don't become footholds and how often those are part of `paths-not-taken'. As participants walk each path twice, we will measure the variability in gaze allocation both with-in and across participants to establish measurements of variability in gaze allocation for individuals with typical visual and motor systems. Establishing the individual variability for those with typical vision is important as previous work demonstrates that an impairment to visual processing leads to the allocation of gaze closer to the body [CITE binocular walking paper]. Thus, changes in gaze allocation may serve as a behavioral biomarker of issues in visual processing.

Relying on the specified gaze locations (e.g., foothold 1, foothold 2, \ldots, non-foothold location, path-not-taken location), we will quantify/model the gaze sequences present during walking in complex terrains. Such sequences can easily be summarized as a markov chain, quantifying the transition probability from one gaze location to the next. We expect that this analysis will capture the broad structure of eye movements in this task. However, we also recognize that the choice of the next gaze location is likely dependent on more than just the current gaze location, a known limitation of this type of Markovian analysis and there are likely common sequences (gaze strategies) that are not captured. Recent work in task-based eye movement analysis has had success in identifying eye movement strategies using Hidden Markov Models (HMMs) and predicting intent using Recurrent Neural Networks (RNNs, specifically LSTM). We will use these modeling approaches to capture the additional complexity in the gaze sequences of walkers.

The larger size of the dataset will offer a unique opportunity to study the sequences of searches on `paths-not-taken' and examine sensorimotor decision-making in the context of path planning. As these events are less common on any individual walk, access to a large dataset is necessary. Preliminary analyses suggest that [FIGURE OUT HOW TO SAY THIS PROPERLY] humans explore their path options in stereotyped ways\ldots{} blah blah blah

ii. \emph{How is gaze allocation modulated by step-to-step gait
efficiency?}

From previous work, we know that terrain difficulty impacts gaze [CITE]. However, many factors change across terrains. In the more difficult terrains, one of central changes to gait is a greater deviation from the preferred gait (i.e., decreased gait efficiency). This accommodates the fewer available footholds due to increased terrain complexity. Here we look to isolate within a terrain the impact of the current level of stability on gaze allocation.

All people have a preferred gait and we can measure each person's preferred gait parameters (e.g., ...) as they walk on flat terrain (pavement). Then we can measure the step-to-step efficiency as the inverted deviation from that preferred gait (see [CITE] for example).  --> (using GLM approaches, with history kernels) relate that deviation to look-ahead distance, look-ahead time and gaze location. We expect that individuals will modulate their gaze to do longer term path planning and foothold finding during periods of instability, i.e. their gaze will be farther down the path during periods of stability.

iii. \emph{What makes a ``good'' foothold?} Foothold locations are
determined by a variety of factors: getting to a location, minimizing the energetic costs of walking, avoiding paths that change in height or direction, stability and/or ``flatness'' of the ground. Basically, you want to get from point A to B, but without getting too tired or stepping on wobbly rocks. How do walkers trade off these different costs? Our preliminary work on this question demonstrates that walkers select paths that are flatter, electing to take more circuitous paths to keep the change in height across steps lower [CITE]. The photogrammetry pipeline was used to build depth maps of the walking terrain and generate viable alternative routes to compare with those chosen by walkers. Additional analyses (using a CNN trained on depth maps) demonstrated that subject-perspective depth maps contain sufficient information to classify foothold locations.

In order to address this question more broadly, we will: 1. Repeat the path height analyses, examining the impact of path height and depth content on foothold selection in our larger dataset, establishing the level of individual variability in the tradeoff between changing path height and more circuitous routes 2. Analyze the role of texture cues (i.e., monocular features) available in the scene on foothold selection. We will train classifiers for foothold and non-foothold locations, using both standard visual processing models (e.g., Portilla-Simoncelli,... 1-2 others) and pre-trained ML models (e.g., VGG, ResNet, ViT) generate an embedding of the visual input. The variety of approaches, combined with the filtering out of different types of information (spatial frequency, orientation, regions of interest) will allow us to determine what types of information can be used by such a classifier to distinguish a good foothold location. 3. Develop a binocular RNN model that identifies good foothold locations in a visual scene, learning from the human data. We will examine how generalizeable models are across individuals. We will look at the role of monocular vs binocular cues by building comparable monocular and perspective depth map versions of the RNN model. Taken together these analyses will build a better understanding of what makes a good foothold and how visual processing supports the selection of good footholds.

v. \emph{How does divided attention impact the visuomotor control of walking?} When humans walk through the world they are frequently doing more than just finding safe footholds. They might also talk to a friend, or check directions on their phone, or be planning out the rest of their day. Divided attention and/or task switching is required in these contexts. This puts limits on the resources available for the visuomotor control of walking. Previous work on .. has found this to \ldots{} Because these contexts are common in everyday life, it is critical that we measure and compare performance to multi-task conditions. In this project we will use two divided attention tasks: 1. Having a conversation with another person and 2. Using a cell phone to play a game or text. Below we briefly describe some of the major hypotheses we have about how divide attention will impact the measured outcomes in our previous research questions (i-iv). 

\emph{[i. Gaze Allocation]} We expect that people will spend less time attending to footholds, allocating some fixations to the task. We will measure how the cost of the second task changes fixation location, fixation duration and fixation sequences. We can further explore this attentional tradeoff by comparing how gaze allocation changes across our two divided attention conditions, which have very different visual information needs. \emph{[ii. Step-to-step gait efficiency]} Is there a decrease in step-to-step efficiency with attention engaged elsewhere? We will measure the step-to-step efficiency observed in the
single task and divided attention conditions. The cost of the second task may decrease step-to-step efficiency, and we are curious how variable this decrease is across individuals and whether it is predicted by the measurement of their dynamic balance (NAME OF TEST). \emph{[iii Foothold properties]} Does the choice of footholds become less precise in quantifiable ways? In the divided attention task the number of fixations allocated to footholds will be decreased. Using classification analyses like those described in (iii) we can ask whether there is discernible difference between the visual properties available during the ground fixations performed in the single task condition and the divided attention conditions. \emph{[iv. Ground Clearance]} Finally we are interested in understanding whether walkers adjust their ground clearance in order to avoid ``scuffs'' or tripping that might be caused by the lowered attention to foothold finding.

vi. [COMMENTS ON MODELING? That point to the end of Aim II]

\noindent \underline{\textbf{Experimental concerns.}}

\begin{itemize}
\item
  This seems hard → Validated in previous published studies that we were
  authors in.
\item
  What if there's learning? → We will complete the walking bouts in a
  randomized order twice. Participants will walk the path for the
  experiment once before. We don't expect measurable effects of learning
  on this time scale but we will be able to check. If the first 10
  participants show evidence of learning effects across sessions we can
  consider collecting data at two sites.
\end{itemize}
