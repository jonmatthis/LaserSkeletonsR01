Our big deal is integrated visuomotor datasets because we are fucking awesome.

\noindent \textbf{\underline{In the wild.}} A major contribution of this grant will be to lower the barrier to the collection and analysis of integrated visuomotor datasets collected in real-world environments.  The is will involve the production of open-hardware, open source data processing pipelines, and a well-documented visuomotor dataset for visually-guided walking:


COPY PASTED FROM PREVIOUS AIM 1 - NOT CHANGED YET
Participants will be recruited and screened to include healthy young adults: ages 18-45 who have normal, or corrected-to-normal vision and typically functioning motor systems (details JON!?!). Screenings will include measurement of visual acuities, stereo-vision, Y balance test (what else). We will recruit a diverse set of participants, advertising our experiment in the IUSO optometry clinic, at Ivy Tech Community College (Bloomington), the public library, at 5 local community/recreation centers and other community bulletin boards.

We will conduct data collection from participants (n=75) over the first 2-3 years of the grant period. Note that the size of the dataset is relatively large for this type of data collection (about one order of magnitude larger than all previous datasets).  This is critical for two main reasons: 1. it will enable us to establish estimates of individual variability for healthy young adults.  2. it will support more effective modeling efforts both by us and by future users of the data set.  In its current state, machine learning and deep learning models typically require huge quantities of data in order to be successful.

The methods we will use in Aim 1 are protocols that the co-PIs developed during their postdoctoral work. These methods are described in detail in [CITE]. Data collection relies on three primary pieces of equipment (see Figure XXx): a mobile eye tracker (Pupil Labs), a body motion suit (Rokoko), and a lightweight computer (e.g., Macbook Air). A 9-point VOR eye tracking calibration procedure [CITE] is used to calibrate the eyetracker and place the body motion data in the same reference frame as the gaze data. Finally, a photogrammetry pipeline, described in detail in [CITE], provides a reconstruction of the environment that individuals walked through and the location of the head-mounted camera moving through the environment. The camera location is used to localize the walker in the reconstructed environment.

As in the previous data collection locations [CITE], we have
identified locations near Indiana University with easy access to
multiple types of terrain: pavement (smooth, flat), medium (pebbles, tree roots), rough (similar to a rocky hiking trail or a dry creekbed), see Figure (XXb). Participants will perform out-and-back walks twice (double-pass).  We expect each walking bout to take 10-15 minutes and for all walking bouts to be completed in one session.

Data analysis will be performed using a set of custom open-source
processing pipelines, written in python. These will integrate the three data streams: eye movements, body movements, world content (see technical deliverables below for more details).

--------- stuff below here needs to be integrated with stuff above.

[Hardware / Data Collection Documentation] We will provide detailed open source documentation for the hardware integration of a motion capture suit (Rokoko), eye tracker (Pupil Labs), and data collection device (tablet/computer). We will also create detailed documentation and video tutorials for how to collect these data.

[Data Processing Pipelines] There are several steps required to convert the raw data from these devices into a calibrated and integrated visuomotor dataset. In our previous work together, we created an initial version of these pipelines. However they are not easily re-usable by other groups or updated for new technologies. Since the creation of those initial pipelines, Matthis (NU)
has had significant training in open source software development. Using our internal pipelines as a starting point we will develop a suite of packages that: 1. Perform photogrammetry on the world video content, relying on Meshroom (https://alicevision.org/). 2. Spatially calibrate the gaze data, body motion and environment into the same reference frame (see methods CITE). 3. Generate visualizations of walkers moving through the terrain (e.g., youtube link). All three packages will be written in
python and shared as open-source packages on Github.  These will become a vital resource for the study of sensorimotor processing.  

[Data] The integrated visuomotor dataset (n=75) collected in Aim 1 will be made available as a data repository on Zenodo. This dataset will include the original raw data as well as the fully processed data that includes the photogrammetry reconstruction and spatially calibrated data.  We will work internally and with collaborators (e.g., Johannes Burge, Eero Simoncelli, ... ?? we could get letters of support) to build an API that not only allows individuals to download the full dataset, but also provides methods to query the dataset for combinations of image, video, biomechanics, and label data well-suited to advanced modelling/ML that is beyond the scope of the current proposal.




\noindent \textbf{\underline{Augmented Reality Ground Plane.}} Some more stuff



\noindent \textbf{\underline{General analysis strategies for integrated visuomotor datasets.}} You guessed it... even more stuff.
