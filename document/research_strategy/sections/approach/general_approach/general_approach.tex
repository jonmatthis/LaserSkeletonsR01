Our big deal is integrated visuomotor datasets because we are fucking awesome.

\noindent \textbf{\underline{In the wild.}} Some stuff.

COPY PASTED FROM PREVIOUS AIM 1 - NOT CHANGED YET
Participants will be recruited and screened to include healthy young adults: ages 18-45 who have normal, or corrected-to-normal vision and typically functioning motor systems (details JON!?!). Screenings will include measurement of visual acuities, stereo-vision, Y balance test (what else). We will recruit a diverse set of participants, advertising our experiment in the IUSO optometry clinic, at Ivy Tech Community College (Bloomington), the public library, at 5 local community/recreation centers and other community bulletin boards.

We will conduct data collection from participants (n=75) over the first 2-3 years of the grant period. Note that the size of the dataset is relatively large for this type of data collection (about one order of magnitude larger than all previous datasets).  This is critical for two main reasons: 1. it will enable us to establish estimates of individual variability for healthy young adults.  2. it will support more effective modeling efforts both by us and by future users of the data set.  In its current state, machine learning and deep learning models typically require huge quantities of data in order to be successful.

The methods we will use in Aim 1 are protocols that the co-PIs developed during their postdoctoral work. These methods are described in detail in [CITE]. Data collection relies on three primary pieces of equipment (see Figure XXx): a mobile eye tracker (Pupil Labs), a body motion suit (Rokoko), and a lightweight computer (e.g., Macbook Air). A 9-point VOR eye tracking calibration procedure [CITE] is used to calibrate the eyetracker and place the body motion data in the same reference frame as the gaze data. Finally, a photogrammetry pipeline, described in detail in [CITE], provides a reconstruction of the environment that individuals walked through and the location of the head-mounted camera moving through the environment. The camera location is used to localize the walker in the reconstructed environment.

As in the previous data collection locations [CITE], we have
identified locations near Indiana University with easy access to
multiple types of terrain: pavement (smooth, flat), medium (pebbles, tree roots), rough (similar to a rocky hiking trail or a dry creekbed), see Figure (XXb). Participants will perform out-and-back walks twice (double-pass) in each of the three conditions (single task, divided attention - talking, divided attention - phone). We expect each walking bout to take 10-15 minutes and for all walking bouts to be completed across two sessions.

Participants will complete walking bouts in all terrains for three
different conditions: single task, divided attention - talking, divided attention - phone. In the single task condition, participants will be instructed to walk along the path. In the two divided attention conditions there will be a second task. For the divided attention - talking condition, an experimenter will walk along behind the participant talking to them about their day and their plans for the weekend. For the divided attention - phone condition we will ask participants to load a web experiment created with Pavlovia/Psychopy [CITE] on their phone to play a puzzle game (like Candy-crush) while they walk along the path.

Data analysis will be performed using a set of custom open-source
processing pipelines, written in python. These will integrate the three data streams: eye movements, body movements, world content (see technical deliverables below for more details).

\noindent \textbf{\underline{Augmented Reality Ground Plane.}} Some more stuff

\noindent \textbf{\underline{General analysis strategies for integrated visuomotor datasets.}} You guessed it... even more stuff.
